# -*- coding: utf-8 -*-
"""
PASO 6 v3: Entrenamiento XGBoost con VARIABLES M√ÅGICAS + VALIDACI√ìN ANTI-OVERFITTING
====================================================================================
Mejoras sobre v2:
1. Variables de INERCIA (lags de temperatura y ocupaci√≥n)
2. Variables de VELOCIDAD DE CAMBIO (diff de temperatura y consumo)
3. RandomizedSearchCV con TimeSeriesSplit para evitar overfitting
4. Comparaci√≥n Train vs Test para detectar memorizaci√≥n
"""

import pandas as pd
import numpy as np
import xgboost as xgb
import joblib
import os
import json
import sys
from datetime import datetime
from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Fix encoding para Windows
sys.stdout.reconfigure(encoding='utf-8')

# ==========================================
# CONFIGURACI√ìN
# ==========================================
BASE_DIR = r"c:\Users\POWER\OneDrive\Escritorio\consumos_uptc_hackday"
INPUT_DIR = os.path.join(BASE_DIR, "DATASETS_ENTRENAMIENTO_LISTOS")
MODEL_DIR = os.path.join(BASE_DIR, "MODELOS_XGBOOST_V3")
RESULTS_DIR = os.path.join(BASE_DIR, "RESULTADOS_ENTRENAMIENTO_V3")

for d in [MODEL_DIR, RESULTS_DIR]:
    os.makedirs(d, exist_ok=True)

# Archivos de entrada
ARCHIVOS_SEDES = {
    'UPTC_TUN': 'train_ready_UPTC_TUN.csv',
    'UPTC_SOG': 'train_ready_UPTC_SOG.csv',
    'UPTC_DUI': 'train_ready_UPTC_DUI.csv',
    'UPTC_CHI': 'train_ready_UPTC_CHI.csv',
}

# ==========================================
# DEFINICI√ìN DEL VECTOR DE ENTRADA (X)
# ==========================================

# A. Temporales C√≠clicas - El "Reloj Matem√°tico"
FEATURES_CICLICAS = [
    'hora_sin', 'hora_cos',           # Ciclo diario (0-24h)
    'dia_sem_sin', 'dia_sem_cos',     # Ciclo semanal (Lun-Dom)
    'mes_sin', 'mes_cos',             # Ciclo anual (Ene-Dic)
]

# B. Calendario Acad√©mico - Contexto Operativo
FEATURES_CALENDARIO = [
    'es_fin_semana',                  # 0/1 - Apaga labs, baja ocupaci√≥n
    'es_festivo',                     # 0/1 - Apagado general (Baseload)
    'periodo_academico_semestre_1',   # 0/1 - Operaci√≥n normal
    'periodo_academico_semestre_2',   # 0/1 - Operaci√≥n normal
]

# C. Variables Ex√≥genas (F√≠sicas)
FEATURES_EXOGENAS = [
    'temperatura_exterior_c',         # Afecta calentadores y eficiencia
    'ocupacion_pct',                  # Proporcional a uso de agua y luz
]

# D. Memoria Hist√≥rica - CR√çTICO PARA XGBOOST
LAGS = [1, 24, 168]  # 1h, 24h (ayer misma hora), 168h (semana pasada mismo d√≠a/hora)
ROLLING_WINDOWS = [24]  # Promedio √∫ltimas 24h

# E. üÜï VARIABLES M√ÅGICAS - Inercia y Velocidad de Cambio
# Estas se generan din√°micamente

# TARGETS a predecir
TARGETS = [
    'energia_total_kwh',
    'energia_comedor_kwh',
    'energia_salones_kwh',
    'energia_laboratorios_kwh',
    'energia_auditorios_kwh',
    'energia_oficinas_kwh',
    'potencia_total_kw',
    'agua_litros',
    'co2_kg',
]

# ==========================================
# HIPERPAR√ÅMETROS - ESPACIO DE B√öSQUEDA
# ==========================================

PARAM_DIST = {
    'n_estimators': [300, 500, 800, 1000],      # ¬øCu√°ntos √°rboles?
    'max_depth': [3, 5, 7, 9],                   # ¬øComplejidad de preguntas?
    'learning_rate': [0.01, 0.03, 0.05, 0.1],   # ¬øVelocidad de aprendizaje?
    'subsample': [0.7, 0.8, 0.9],               # % datos por √°rbol (anti-overfit)
    'colsample_bytree': [0.7, 0.8, 0.9],        # % features por √°rbol
    'min_child_weight': [3, 5, 7],              # M√≠nimo por hoja (regularizaci√≥n)
    'reg_alpha': [0, 0.01, 0.1, 1],             # L1 regularization
    'reg_lambda': [1, 1.5, 2, 3],               # L2 regularization
}

# Configuraci√≥n de validaci√≥n cruzada
N_ITER_SEARCH = 15           # Combinaciones a probar (balance tiempo/exploraci√≥n)
N_SPLITS_CV = 5              # Particiones temporales para TimeSeriesSplit
TEST_SIZE = 0.2              # Holdout final

# Umbral para detectar overfitting
OVERFITTING_THRESHOLD = 0.10  # Si R2_train - R2_test > 0.10, hay overfitting

# ==========================================
# FUNCIONES DE FEATURE ENGINEERING
# ==========================================

def agregar_variables_magicas(df):
    """
    Agrega variables de INERCIA y VELOCIDAD DE CAMBIO.
    
    INERCIA: El efecto retardado
    - La temperatura de hace 1h afecta el consumo de AHORA
    - La ocupaci√≥n de hace 1h indica si la gente est√° llegando
    
    VELOCIDAD DE CAMBIO: Tendencias
    - ¬øLa temperatura est√° subiendo o bajando?
    - ¬øEl consumo est√° acelerando?
    
    Returns:
        DataFrame con nuevas columnas m√°gicas
    """
    df = df.copy()
    
    # =====================
    # 1. EFECTO RETARDADO (INERCIA)
    # =====================
    # La temperatura de hace 1 hora afecta el consumo de AHORA
    if 'temperatura_exterior_c' in df.columns:
        df['temp_hace_1h'] = df['temperatura_exterior_c'].shift(1)
        df['temp_hace_3h'] = df['temperatura_exterior_c'].shift(3)
    
    # La ocupaci√≥n de hace 1 hora nos dice si la gente est√° llegando
    if 'ocupacion_pct' in df.columns:
        df['ocupacion_hace_1h'] = df['ocupacion_pct'].shift(1)
        df['ocupacion_hace_3h'] = df['ocupacion_pct'].shift(3)
    
    # =====================
    # 2. VELOCIDAD DE CAMBIO (TENDENCIA)
    # =====================
    # ¬øLa temperatura est√° subiendo o bajando?
    if 'temperatura_exterior_c' in df.columns:
        df['cambio_temp_1h'] = df['temperatura_exterior_c'].diff(1)
        df['cambio_temp_3h'] = df['temperatura_exterior_c'].diff(3)
    
    # ¬øLa ocupaci√≥n est√° creciendo?
    if 'ocupacion_pct' in df.columns:
        df['cambio_ocupacion_1h'] = df['ocupacion_pct'].diff(1)
    
    return df


def generar_features_lag(df, target_col, lags=[1, 24, 168]):
    """
    Genera features de lag para una columna target.
    """
    df = df.copy()
    
    for lag in lags:
        col_name = f'{target_col}_lag_{lag}h'
        df[col_name] = df[target_col].shift(lag)
    
    return df


def generar_features_rolling(df, target_col, windows=[24]):
    """
    Genera features de rolling mean para una columna target.
    """
    df = df.copy()
    
    for window in windows:
        col_name = f'{target_col}_rolling_mean_{window}h'
        df[col_name] = df[target_col].rolling(window=window, min_periods=1).mean().shift(1)
    
    return df


def generar_velocidad_consumo(df, target_col):
    """
    Genera la velocidad de cambio del consumo (tendencia del target).
    """
    df = df.copy()
    col_name = f'{target_col}_velocidad_1h'
    df[col_name] = df[target_col].diff(1)
    return df


def preparar_features(df, target_col):
    """
    Prepara el DataFrame con TODAS las features, incluyendo variables m√°gicas.
    
    Args:
        df: DataFrame original
        target_col: Nombre del target
    
    Returns:
        X: Features
        y: Target  
        feature_names: Lista de nombres de features
        df_clean: DataFrame limpio
    """
    df = df.copy()
    
    # 1. Agregar VARIABLES M√ÅGICAS (inercia y velocidad)
    df = agregar_variables_magicas(df)
    
    # 2. Generar features de memoria hist√≥rica para este target
    df = generar_features_lag(df, target_col, LAGS)
    df = generar_features_rolling(df, target_col, ROLLING_WINDOWS)
    df = generar_velocidad_consumo(df, target_col)
    
    # Nombres de las features de memoria
    lag_features = [f'{target_col}_lag_{lag}h' for lag in LAGS]
    rolling_features = [f'{target_col}_rolling_mean_{w}h' for w in ROLLING_WINDOWS]
    velocidad_features = [f'{target_col}_velocidad_1h']
    
    # Lista de features m√°gicas
    magic_features = []
    
    # Inercia t√©rmica
    if 'temp_hace_1h' in df.columns:
        magic_features.extend(['temp_hace_1h', 'temp_hace_3h'])
    
    # Inercia de ocupaci√≥n  
    if 'ocupacion_hace_1h' in df.columns:
        magic_features.extend(['ocupacion_hace_1h', 'ocupacion_hace_3h'])
    
    # Velocidad de cambio
    if 'cambio_temp_1h' in df.columns:
        magic_features.extend(['cambio_temp_1h', 'cambio_temp_3h'])
    
    if 'cambio_ocupacion_1h' in df.columns:
        magic_features.append('cambio_ocupacion_1h')
    
    # Construir lista completa de features
    all_features = []
    
    # A. Temporales c√≠clicas
    for f in FEATURES_CICLICAS:
        if f in df.columns:
            all_features.append(f)
    
    # B. Calendario acad√©mico
    for f in FEATURES_CALENDARIO:
        if f in df.columns:
            all_features.append(f)
    
    # Buscar columnas de vacaciones
    vacaciones_cols = [c for c in df.columns if 'vacaciones' in c.lower()]
    for f in vacaciones_cols:
        if f not in all_features:
            all_features.append(f)
    
    # C. Variables ex√≥genas
    for f in FEATURES_EXOGENAS:
        if f in df.columns:
            all_features.append(f)
    
    # D. Features de memoria hist√≥rica
    all_features.extend(lag_features)
    all_features.extend(rolling_features)
    
    # E. üÜï VARIABLES M√ÅGICAS
    all_features.extend(magic_features)
    all_features.extend(velocidad_features)
    
    # Eliminar filas con NaN (las primeras 168 horas por el lag de 1 semana)
    df_clean = df.dropna(subset=all_features + [target_col])
    
    X = df_clean[all_features].values
    y = df_clean[target_col].values
    
    return X, y, all_features, df_clean


# ==========================================
# FUNCIONES DE M√âTRICAS Y EVALUACI√ìN
# ==========================================

def calcular_metricas(y_true, y_pred):
    """Calcula m√©tricas de evaluaci√≥n."""
    mask = y_true != 0
    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100 if mask.sum() > 0 else np.inf
    
    return {
        'RMSE': float(np.sqrt(mean_squared_error(y_true, y_pred))),
        'MAE': float(mean_absolute_error(y_true, y_pred)),
        'R2': float(r2_score(y_true, y_pred)),
        'MAPE': float(min(mape, 999.99)),  # Cap para evitar infinitos en JSON
    }


def interpretar_r2(r2):
    """Interpreta el valor de R2."""
    if r2 >= 0.90:
        return "üèÜ EXCELENTE"
    elif r2 >= 0.80:
        return "‚úÖ MUY BUENO"
    elif r2 >= 0.70:
        return "üëç BUENO"
    elif r2 >= 0.50:
        return "‚ö†Ô∏è ACEPTABLE"
    else:
        return "‚ùå DEFICIENTE"


def detectar_overfitting(r2_train, r2_test, threshold=OVERFITTING_THRESHOLD):
    """
    Detecta si hay overfitting comparando R2 de train vs test.
    
    Un modelo que memoriza tendr√° R2_train muy alto pero R2_test bajo.
    """
    gap = r2_train - r2_test
    
    if gap > threshold:
        return True, gap, f"‚ö†Ô∏è OVERFITTING (gap={gap:.3f})"
    elif gap > threshold/2:
        return False, gap, f"üëÄ Vigilar (gap={gap:.3f})"
    else:
        return False, gap, f"‚úÖ OK (gap={gap:.3f})"


def obtener_feature_importance(model, feature_names, top_n=10):
    """Obtiene las caracter√≠sticas m√°s importantes."""
    importance = model.feature_importances_
    indices = np.argsort(importance)[::-1][:top_n]
    
    top_features = []
    for i in indices:
        top_features.append({
            'feature': feature_names[i],
            'importance': float(importance[i])
        })
    return top_features


# ==========================================
# B√öSQUEDA DE HIPERPAR√ÅMETROS
# ==========================================

def buscar_mejores_hiperparametros(X_train, y_train, n_iter=N_ITER_SEARCH, n_splits=N_SPLITS_CV):
    """
    Busca los mejores hiperpar√°metros usando RandomizedSearchCV con TimeSeriesSplit.
    
    TimeSeriesSplit es CR√çTICO para series temporales porque:
    - NUNCA usa datos del futuro para predecir el pasado
    - Cada fold usa datos anteriores para entrenar y siguientes para validar
    
    Returns:
        best_model: Mejor modelo encontrado
        best_params: Mejores hiperpar√°metros
        cv_score: Score de validaci√≥n cruzada
    """
    # Modelo base
    xgb_base = xgb.XGBRegressor(
        objective='reg:squarederror',
        n_jobs=-1,
        random_state=42,
        verbosity=0
    )
    
    # TimeSeriesSplit respeta el orden temporal
    tscv = TimeSeriesSplit(n_splits=n_splits)
    
    # B√∫squeda randomizada
    search = RandomizedSearchCV(
        estimator=xgb_base,
        param_distributions=PARAM_DIST,
        n_iter=n_iter,
        scoring='neg_root_mean_squared_error',  # Negativo porque sklearn minimiza
        cv=tscv,
        verbose=0,
        n_jobs=-1,
        random_state=42
    )
    
    search.fit(X_train, y_train)
    
    return search.best_estimator_, search.best_params_, -search.best_score_


# ==========================================
# ENTRENAMIENTO PRINCIPAL
# ==========================================

def entrenar_modelos(usar_busqueda_hp=True):
    """
    Entrena modelos XGBoost con:
    1. Variables m√°gicas (inercia y velocidad)
    2. B√∫squeda de hiperpar√°metros (opcional)
    3. Validaci√≥n anti-overfitting
    
    Args:
        usar_busqueda_hp: Si True, usa RandomizedSearchCV. Si False, usa par√°metros fijos.
    """
    
    print("=" * 70)
    print("üöÄ ENTRENAMIENTO XGBoost v3 - VARIABLES M√ÅGICAS + ANTI-OVERFITTING")
    print("=" * 70)
    print(f"üìÖ Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"\nüìä MEJORAS EN ESTA VERSI√ìN:")
    print(f"   üßä Variables de INERCIA: temp_hace_1h, temp_hace_3h, ocupacion_hace_1h")
    print(f"   ‚ö° Variables de VELOCIDAD: cambio_temp_1h, velocidad_consumo")
    print(f"   üîç Detecci√≥n de OVERFITTING: Comparaci√≥n Train vs Test")
    if usar_busqueda_hp:
        print(f"   üéØ B√∫squeda hiperpar√°metros: RandomizedSearchCV ({N_ITER_SEARCH} iter)")
        print(f"   ‚è±Ô∏è  TimeSeriesSplit: {N_SPLITS_CV} folds (respeta orden temporal)")
    
    resultados_globales = {
        'fecha_entrenamiento': datetime.now().isoformat(),
        'version': 'v3_magic_features_anti_overfit',
        'configuracion': {
            'usar_busqueda_hp': usar_busqueda_hp,
            'n_iter_search': N_ITER_SEARCH if usar_busqueda_hp else 0,
            'n_splits_cv': N_SPLITS_CV,
            'overfitting_threshold': OVERFITTING_THRESHOLD,
        },
        'variables_magicas': [
            'temp_hace_1h', 'temp_hace_3h',
            'ocupacion_hace_1h', 'ocupacion_hace_3h',
            'cambio_temp_1h', 'cambio_temp_3h',
            'cambio_ocupacion_1h',
            '{target}_velocidad_1h'
        ],
        'sedes': {}
    }
    
    total_modelos = 0
    modelos_exitosos = 0
    modelos_con_overfitting = 0
    
    for sede, archivo in ARCHIVOS_SEDES.items():
        input_path = os.path.join(INPUT_DIR, archivo)
        
        if not os.path.exists(input_path):
            print(f"\n‚ö†Ô∏è  Saltando {sede}: No existe {input_path}")
            continue
        
        print(f"\n{'='*70}")
        print(f"üìç SEDE: {sede}")
        print(f"{'='*70}")
        
        # Cargar datos
        df = pd.read_csv(input_path)
        
        # Asegurar orden temporal
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.sort_values('timestamp').reset_index(drop=True)
        
        print(f"   üìä Datos cargados: {len(df):,} filas √ó {len(df.columns)} columnas")
        
        resultados_globales['sedes'][sede] = {
            'archivo': archivo,
            'n_registros': len(df),
            'modelos': {}
        }
        
        for target in TARGETS:
            if target not in df.columns:
                print(f"   ‚ö†Ô∏è  Target '{target}' no encontrado, saltando...")
                continue
            
            total_modelos += 1
            
            print(f"\n   {'‚îÄ'*50}")
            print(f"   üéØ TARGET: {target}")
            print(f"   {'‚îÄ'*50}")
            
            # Preparar features con variables m√°gicas
            try:
                X, y, feature_names, df_clean = preparar_features(df, target)
            except Exception as e:
                print(f"      ‚ùå ERROR preparando features: {e}")
                continue
            
            n_features = len(feature_names)
            n_samples = len(X)
            
            print(f"      üìê Features: {n_features} | Muestras: {n_samples:,}")
            
            # Contar variables m√°gicas
            magic_count = sum(1 for f in feature_names if 
                            'hace_' in f or 'cambio_' in f or 'velocidad' in f)
            print(f"      ‚ú® Variables m√°gicas activas: {magic_count}")
            
            # Divisi√≥n temporal (NUNCA shuffle en series de tiempo)
            split_idx = int(len(X) * (1 - TEST_SIZE))
            X_train, X_test = X[:split_idx], X[split_idx:]
            y_train, y_test = y[:split_idx], y[split_idx:]
            
            print(f"      üìà Train: {len(X_train):,} | Test: {len(X_test):,}")
            
            # ==========================================
            # ENTRENAMIENTO CON O SIN B√öSQUEDA HP
            # ==========================================
            
            if usar_busqueda_hp:
                print(f"      üîç Buscando hiperpar√°metros...")
                model, best_params, cv_rmse = buscar_mejores_hiperparametros(X_train, y_train)
                print(f"      ‚úÖ Mejor RMSE CV: {cv_rmse:.4f}")
            else:
                # Par√°metros fijos (m√°s r√°pido)
                best_params = {
                    'n_estimators': 500,
                    'max_depth': 7,
                    'learning_rate': 0.05,
                    'subsample': 0.8,
                    'colsample_bytree': 0.8,
                    'min_child_weight': 5,
                    'reg_alpha': 0.1,
                    'reg_lambda': 1.5,
                }
                model = xgb.XGBRegressor(
                    objective='reg:squarederror',
                    n_jobs=-1,
                    random_state=42,
                    verbosity=0,
                    **best_params
                )
                model.fit(X_train, y_train)
            
            # Predicciones
            y_pred_train = model.predict(X_train)
            y_pred_test = model.predict(X_test)
            
            # M√©tricas
            metrics_train = calcular_metricas(y_train, y_pred_train)
            metrics_test = calcular_metricas(y_test, y_pred_test)
            
            # ==========================================
            # DETECCI√ìN DE OVERFITTING
            # ==========================================
            is_overfit, gap, overfit_msg = detectar_overfitting(
                metrics_train['R2'], 
                metrics_test['R2']
            )
            
            if is_overfit:
                modelos_con_overfitting += 1
            
            # Feature importance
            top_features = obtener_feature_importance(model, feature_names, top_n=7)
            
            # ==========================================
            # MOSTRAR RESULTADOS
            # ==========================================
            print(f"      üìä M√âTRICAS:")
            print(f"         Train R¬≤: {metrics_train['R2']:.4f}")
            print(f"         Test R¬≤:  {metrics_test['R2']:.4f} {interpretar_r2(metrics_test['R2'])}")
            print(f"         MAPE:     {metrics_test['MAPE']:.2f}%")
            print(f"         üî¨ Overfitting: {overfit_msg}")
            
            print(f"      üîù TOP FEATURES:")
            for i, feat in enumerate(top_features[:5], 1):
                is_magic = '‚ú®' if ('hace_' in feat['feature'] or 
                                   'cambio_' in feat['feature'] or 
                                   'velocidad' in feat['feature']) else ''
                bar = "‚ñà" * int(feat['importance'] * 40)
                print(f"         {i}. {feat['feature'][:25]:<25} {bar} {is_magic}")
            
            # Guardar modelo
            model_filename = f"xgb_v3_{sede}_{target}.pkl"
            model_path = os.path.join(MODEL_DIR, model_filename)
            joblib.dump({
                'model': model,
                'feature_names': feature_names,
                'target': target,
                'sede': sede,
                'lags': LAGS,
                'rolling_windows': ROLLING_WINDOWS,
                'best_params': best_params,
                'version': 'v3_magic_features',
            }, model_path)
            print(f"      üíæ Modelo guardado: {model_filename}")
            
            # Almacenar resultados
            resultados_globales['sedes'][sede]['modelos'][target] = {
                'metricas_train': metrics_train,
                'metricas_test': metrics_test,
                'overfitting': {
                    'detected': is_overfit,
                    'gap': float(gap),
                    'message': overfit_msg
                },
                'best_params': best_params,
                'top_features': top_features,
                'n_features': n_features,
                'n_magic_features': magic_count,
                'n_samples_train': len(X_train),
                'n_samples_test': len(X_test),
                'archivo_modelo': model_filename
            }
            
            modelos_exitosos += 1
    
    # ==========================================
    # RESUMEN FINAL
    # ==========================================
    print("\n" + "=" * 70)
    print("üìä RESUMEN DE ENTRENAMIENTO v3 (MAGIC FEATURES + ANTI-OVERFIT)")
    print("=" * 70)
    
    # Tabla resumen R2
    print("\nüèÜ R¬≤ TEST POR SEDE Y TARGET:")
    print("-" * 90)
    header = f"{'Target':<25}"
    for sede in ARCHIVOS_SEDES.keys():
        header += f" | {sede:>10}"
    print(header)
    print("-" * 90)
    
    for target in TARGETS:
        row = f"{target:<25}"
        for sede in ARCHIVOS_SEDES.keys():
            if sede in resultados_globales['sedes']:
                if target in resultados_globales['sedes'][sede]['modelos']:
                    r2 = resultados_globales['sedes'][sede]['modelos'][target]['metricas_test']['R2']
                    overfit = resultados_globales['sedes'][sede]['modelos'][target]['overfitting']['detected']
                    marker = "‚ö†Ô∏è" if overfit else ""
                    row += f" | {r2:>8.4f}{marker}"
                else:
                    row += f" | {'N/A':>10}"
            else:
                row += f" | {'N/A':>10}"
        print(row)
    
    print("-" * 90)
    
    # Resumen overfitting
    print(f"\nüî¨ AN√ÅLISIS DE OVERFITTING:")
    print(f"   Total modelos: {modelos_exitosos}")
    print(f"   Con overfitting detectado: {modelos_con_overfitting}")
    print(f"   Tasa de overfitting: {modelos_con_overfitting/max(modelos_exitosos,1)*100:.1f}%")
    
    if modelos_con_overfitting > 0:
        print(f"   ‚ö†Ô∏è Los modelos con overfitting podr√≠an beneficiarse de m√°s regularizaci√≥n")
    else:
        print(f"   ‚úÖ Ning√∫n modelo muestra se√±ales claras de overfitting")
    
    # Guardar resultados
    results_file = os.path.join(RESULTS_DIR, "resultados_entrenamiento_v3.json")
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(resultados_globales, f, indent=2, ensure_ascii=False)
    
    print(f"\n‚úÖ ENTRENAMIENTO COMPLETADO")
    print(f"   üìà Modelos entrenados: {modelos_exitosos}/{total_modelos}")
    print(f"   üíæ Modelos guardados en: {MODEL_DIR}")
    print(f"   üìä Resultados: {results_file}")
    
    # Mejores modelos
    print("\nüèÖ MEJORES MODELOS (R¬≤ m√°s alto por sede):")
    for sede in ARCHIVOS_SEDES.keys():
        if sede in resultados_globales['sedes']:
            modelos = resultados_globales['sedes'][sede].get('modelos', {})
            if modelos:
                mejor = max(modelos.items(), key=lambda x: x[1]['metricas_test']['R2'])
                r2 = mejor[1]['metricas_test']['R2']
                print(f"   {sede}: {mejor[0]} (R¬≤={r2:.4f})")
    
    return resultados_globales


# ==========================================
# FUNCI√ìN PARA PREDICCIONES
# ==========================================

def predecir(sede, target, df_nuevos_datos):
    """
    Realiza predicciones usando el modelo v3 entrenado.
    
    IMPORTANTE: df_nuevos_datos debe tener historial suficiente para:
    - Lags: H-1, H-24, H-168
    - Variables m√°gicas: temperatura y ocupaci√≥n hist√≥ricas
    """
    model_path = os.path.join(MODEL_DIR, f"xgb_v3_{sede}_{target}.pkl")
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Modelo no encontrado: {model_path}")
    
    # Cargar modelo y metadatos
    data = joblib.load(model_path)
    model = data['model']
    feature_names = data['feature_names']
    lags = data['lags']
    rolling_windows = data['rolling_windows']
    
    # Preparar features
    df = df_nuevos_datos.copy()
    df = agregar_variables_magicas(df)
    df = generar_features_lag(df, target, lags)
    df = generar_features_rolling(df, target, rolling_windows)
    df = generar_velocidad_consumo(df, target)
    
    # Verificar features
    missing = [f for f in feature_names if f not in df.columns]
    if missing:
        raise ValueError(f"Faltan features: {missing}")
    
    X = df[feature_names].values
    predicciones = model.predict(X)
    
    return predicciones


# ==========================================
# EJECUCI√ìN
# ==========================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Entrenamiento XGBoost v3')
    parser.add_argument('--fast', action='store_true', 
                       help='Modo r√°pido sin b√∫squeda de hiperpar√°metros')
    args = parser.parse_args()
    
    usar_busqueda = not args.fast
    
    if args.fast:
        print("üèÉ Modo R√ÅPIDO: Sin b√∫squeda de hiperpar√°metros")
    else:
        print("üîç Modo COMPLETO: Con RandomizedSearchCV (m√°s lento pero mejor)")
    
    resultados = entrenar_modelos(usar_busqueda_hp=usar_busqueda)
